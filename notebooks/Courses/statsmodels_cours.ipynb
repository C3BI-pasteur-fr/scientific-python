{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d2cf1-9281-4a58-9dcd-3b0b1ab7cb0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!\"{sys.executable}\" -m pip install statsmodels\n",
    "import statsmodels_material"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b66572-e886-4b73-9a82-2f26a1295280",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; margin-top: 50px; margin-bottom: 50px\"><img alt=\"StatsModels logo\" src=\"../images/statsmodels-logo-v2-horizontal.svg\" width=\"60%\" /></div>\n",
    "\n",
    "The statsmodels library provides utilities for the design of linear models of one or more response (or dependent) variables as a function of explanatory (or independent) variables.\n",
    "\n",
    "It features many modules. We will import two of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6590258-e1ac-4f62-8adf-3fa014376a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4f63c1-9995-4516-aeb5-1fb4fe1db896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import pingouin as pg\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047523b9-169d-487c-b4a2-ed29aa6d0e16",
   "metadata": {},
   "source": [
    "For example, we will specify linear models using Wilkinson formulae, *e.g.*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b07451-6f60-43b3-980b-1e8ebd46baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pg.read_dataset('anova3')\n",
    "df.loc[range(0, df.shape[0], 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b44ca4b-baa1-4210-85ce-5284e0f320db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.ols('Cholesterol ~ Sex * Risk * Drug', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c651c104-a610-40a4-b4ec-bcd6c46f3c07",
   "metadata": {},
   "source": [
    "We will also see several criteria for determining whether a model adequately fits the data, as well as for choosing between multiple candidate models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b642a0a-0046-4a61-87d6-d303d41d3f06",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c1d50d-d4f2-4ca2-bb73-7c0613069a2d",
   "metadata": {},
   "source": [
    "Similarly to Pingouin, statsmodels relies of the so-called *long* format, *i.e.* the data are expected to be organized in a DataFrame with one row = one observation, and each variable (be it dependent or independent, categorical or continuous) as a column.\n",
    "\n",
    "To convert groups of observations, *e.g.* three groups of a single measurement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec2f6b1-c4fc-465e-9434-b32333770e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [85, 86, 88, 75, 78, 94, 98, 79, 71, 80]\n",
    "B = [91, 92, 93, 85, 87, 84, 82, 88, 95, 96]\n",
    "C = [79, 78, 88, 94, 92, 85, 83, 85, 82, 81]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa48a3-1623-413b-b49d-1777b8ef92c9",
   "metadata": {},
   "source": [
    "we concatenate the measurements into a single column, and the group information as replicated values in a second column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ff83f-d827-4513-9801-51488e5a1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.concatenate((A, B, C))\n",
    "Group = np.repeat(['A', 'B', 'C'], (len(A), len(B), len(C)))\n",
    "Y, Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c324bd0f-e769-4a0d-8e6b-d4d346e76f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(dict(Y=Y, Group=Group))\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef470a-4628-468d-a32a-af36dcb21881",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "## One-way ANOVA (again)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72b28d4-e4df-4ee3-b4c3-5ec1ef85d1f6",
   "metadata": {},
   "source": [
    "To perform a one-way ANOVA on the above data, we can use tools we already saw:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a597c0ef-75e7-47e1-a959-e7a7324b02e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.f_oneway(A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c687e7-e9b7-44f0-8148-d93bb8605505",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg.anova(dataframe, dv='Y', between='Group')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21969382-f0b9-49e5-90f8-b8d31ca64a49",
   "metadata": {},
   "source": [
    "We now have a third way of doing the same analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2a00ec-43c6-4299-82e3-15b807110828",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fitted_model = smf.ols('Y ~ Group', data=dataframe).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf5c1d-caa3-4072-bc86-ce860b8f33c8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sm.stats.anova_lm(fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64759d8f-7fee-405c-84f2-207fd8786904",
   "metadata": {
    "hidden": true
   },
   "source": [
    "statsmodels - in particular the functions from the `statsmodels.formula.api` module - understands Wilkinson formulae.\n",
    "\n",
    "With expression `Y ~ Group`, we designated *Y* as the *dependent variable* or *response variable* in our analysis, and told the `ols` function to build a linear model of the effect of the `Group` categorical variable on `Y`. OLS stands for *ordinary least squares*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa80f8df-1f58-4de0-be95-a551d27e07eb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "\n",
    "In the summary table, we find again the same *F* statistic and corresponding *p*-value as with `scipy.stats.f_oneway` or `pingouin.anova`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638bdd6b-6964-4209-b762-2991fd3fb7fc",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats.f_oneway(A, B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64a5b9a-9214-4ecc-ad70-2505c5b9e78c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To get a more detailed output about the fitted model, instead of the `statsmodels.api.stats.anova_lm` function we can use the `summary` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe841a0-21b1-4c92-a767-c4ab3abd37f8",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fitted_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6d455b-bbf5-476e-86b3-b513cd4967a4",
   "metadata": {},
   "source": [
    "* The first table gives several indicators of how well the model fits the data.\n",
    "* The second table shows the coefficients (intercept and slopes) of the model, and per-coefficient statistical information.\n",
    "* The third table lists a few tests for additional properties (normality, skewness, kurtosis, etc.).\n",
    "\n",
    "To understand the model, let us have a look at the second table first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1e8107-18e7-4627-83af-eada7e8dcfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274f1098-05ae-46d3-82ea-58c71d786cfb",
   "metadata": {},
   "source": [
    "In the present case, the intercept encodes group A's mean, while the `Group[T.B]` term is the difference between group B's mean and the intercept (or group A's mean), and the `Group[T.C]` term is the difference between group C's mean and the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aca144-0aa3-45f3-82ff-2b9d5922437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = fitted_model.params\n",
    "coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a500b3-c264-42a2-ac09-faa0eb07aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept, B_term, C_term = coefficients\n",
    "ax = sns.boxplot(x='Group', y='Y', data=dataframe)\n",
    "ax.plot(\n",
    "    ax.get_xticks(), # abscissa of the boxplots\n",
    "    np.array([\n",
    "        intercept,            # A mean\n",
    "        intercept + B_term,   # B mean\n",
    "        intercept + C_term]), # C mean\n",
    "    'rs'); # red squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9645b7a5-70f7-4f01-9135-902ab55d9dcf",
   "metadata": {},
   "source": [
    "The associated statistical is of limited interest here.\n",
    "\n",
    "Let us instead focus on the omnibus statistic and other fitness measurements in the first table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d47be2-6e62-4fe7-b56f-bb47e951aca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model.summary().tables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30bf249-4720-488e-a5d4-39497ca1e1c1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The residuals are what the model cannot account for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b2750a-8965-4769-b053-cd95e3583320",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "statsmodels_material.illustration_residuals(dataframe, fitted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158674d4-7123-4f60-b1fa-887f81261816",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model.resid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248559af",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The model effectively catches the group means of $Y$, which results in estimates $\\hat{y}_i$ for each observation $y_i$ and a residual $\\epsilon_i = y_i - \\hat{y}_i$.\n",
    "\n",
    "We are given the $R^2$ and adjusted $R_{adj}^2$:\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_i\\epsilon_i^2}{SS_{total}} \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad R_{adj}^2 = 1 - \\frac{(n-1)(1 - R^2)}{n-k-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30928ea-89bd-40d1-aabf-93c9ff35686c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The last table displays a few statistics about the residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc687c84-7ad2-4055-b00d-efbcb4545ea2",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(fitted_model.summary().tables[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dc6aff-1529-4114-812b-8b802f857f4e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For example, we find mentions of an [Omnibus test of normality](https://www.statsmodels.org/stable/generated/statsmodels.stats.stattools.omni_normtest.html) (*Omnibus*) and the [Jarque-Bera test of normality](https://www.statsmodels.org/stable/generated/statsmodels.stats.stattools.jarque_bera.html) (*JB*), and intermediate measurements of skewness (*Skew*) and kurtosis (*Kurtosis*).\n",
    "The so-called omnibus test is actually the D'Agostino-Pearson test (`scipy.stats.normaltest`) applied to the residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb4ddd-d9f4-42cb-8611-0f0d69da4972",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stats.normaltest(fitted_model.resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e9651e-9e9a-4008-a545-5df63f30706a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note that, here, the kurtosis is estimated as $\\beta_2$ and its expected value for a normal distribution is $3$.\n",
    "\n",
    "The [Durbin-Watson statistic](https://www.statsmodels.org/stable/generated/statsmodels.stats.stattools.durbin_watson.html) quantifies the autocorrelation of the residuals.\n",
    "This statistic takes values in the $[0,4]$ range, it should be as close as possible to $2$, and informs about the homoscedasticity (=equality of variance) of the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a66725-297c-45c1-b262-1a62b3ebd49f",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "## Model specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c1c21f-c77b-478f-a310-af3cc91b5262",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The *response variable* `Y` is approximated as $a + b * \\mathbb{1}_B + c * \\mathbb{1}_C$ denoting\n",
    "$a$, $b$ and $c$ the three coefficients that appear in the `coef` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02834d6d-cc6a-4746-9f2e-443c10a65fd3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(fitted_model.summary().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa935207-cb37-4ee2-9562-ba5852230881",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Design matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d30cc1-196b-49ec-9548-07384aa970e1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The linear model relies on a specific representation of the data: the `endog` and `exog` *design* matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86386730-0784-4304-bb82-c8909e7ffb01",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from patsy import dmatrices\n",
    "endog, exog = dmatrices('Y ~ Group', dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c885739b-16dc-4269-b0cf-1f0379e6c4c8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(statsmodels_material.side_by_side(endog, exog))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207b6cb4-e44f-4ef5-b85b-2670680fa742",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\require{color}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7ee778-b453-4890-ae89-c7453f1a0b57",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The right-hand side (`endog`) is a vector that represents the response variable we previously called *Y*.\n",
    "Here, this is a vector because we model a single response variable.\n",
    "\n",
    "The left-hand side is the (main) *design matrix* (`exog`) and represents the terms involved as input to the linear model.\n",
    "As already said, fitting such a model consists in finding $a$, $b$ and $c$ such that:\n",
    "\n",
    "$$\n",
    "\\mathtt{\\colorbox{#F2F3F4}{Y}} = a \\mbox{ } \\mathtt{\\colorbox{#F2F3F4}{Intercept}} + b \\mbox{ } \\mathtt{\\colorbox{#F2F3F4}{Group[T.B]}} + c \\mbox{ } \\mathtt{\\colorbox{#F2F3F4}{Group[T.C]}} + \\epsilon\n",
    "$$\n",
    "\n",
    "As the intercept is a constant, the corresponding term is always modelled as a constant vector.\n",
    "\n",
    "We can observe that the `Group` variable is represented as several binary variables; one per level of the original categorical variable, **minus one**.\n",
    "These binary variables are called *dummy variables*. All categorical variables are translated this way, into one or several dummy variables.\n",
    "\n",
    "`A` is not explicitly modelled, because all the values in a `Group[T.A]` column could be predicted knowing the corresponding values in the other two `Group` columns.\n",
    "In other words, a `Group[T.A]` dummy variable would not bring additional information.\n",
    "\n",
    "Basically, `A` is taken as a reference group. The intercept is enough to capture group `A`'s mean, and the `Group[T.B]` and `Group[T.C]` variables encodes the offsets with group `A`'s mean for the other 2 groups.\n",
    "\n",
    "If we force `ols` to explicitly use an additional dummy variable for group `A`, designing the matrices ourselves, we get correct output in this case, but `ols` complains about collinearity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ab66b2-b6aa-4437-8427-e3e731fd58e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept, dummyB, dummyC = exog.T\n",
    "dummyA = intercept - dummyB - dummyC\n",
    "overdefined_exog = np.stack((intercept, dummyA, dummyB, dummyC), axis=1)\n",
    "overdefined_exog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18356ea-7df2-4596-abe4-b6a320280809",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "overdefined_model = sm.OLS(endog, overdefined_exog).fit()\n",
    "overdefined_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4cb32f-4ce4-4009-9cbd-528bfcdaa662",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[More about design matrices](https://en.wikipedia.org/wiki/Design_matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab029c-14e4-4730-9a8b-58542413f571",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Wilkinson formulae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa769b95-2744-470c-8213-2a522f39de25",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Wilkinson formulae were introduced in the S language and popularized with the advent of the R language.\n",
    "\n",
    "In Python, this formalism is implemented by the [patsy](https://patsy.readthedocs.io/en/latest/formulas.html) package, required by statsmodels, with [minor differences](https://patsy.readthedocs.io/en/latest/R-comparison.html#r-comparison) with R.\n",
    "\n",
    "As categorical variables may be encoded as numerical values -- in which case patsy cannot guess these variables are categorical, it is good practice to always tag these variables as categorical with the `C()` function in the formula, *e.g.* `C(Group)`.\n",
    "\n",
    "The intercept is implicit; `Y ~ X` and `Y ~ 1 + X` are equivalent formulae. The intercept can be excluded making its contribution negative or representing it with an explicit zero: `Y ~ X - 1` or `Y ~ 0 + X`.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de4753-0da7-4fe9-8412-eff18d66accf",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "del C # more about this later...\n",
    "fitted_model = smf.ols('Y ~ C(Group) - 1', data=dataframe).fit()\n",
    "print(fitted_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28fe336-37b9-4fa4-8670-1416e61aaf24",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<hr />\n",
    "\n",
    "The regression coefficients are also implicit. Although `Y ~ X` corresponds to $Y = a + bX + \\epsilon$, we do not write `Y ~ a + b * X`.\n",
    "`a` and `b` are unknowns and are to be inferred. The terms in the formula should be column names in the dataframe.\n",
    "\n",
    "We can introduce multiple independent variables, *i.e.* several terms, and -- optionally -- their *interactions*: `Y ~ A + B + A:B`, `Y ~ X1 + X2 + X3 + X1:X2 + X1:X3 + X2:X3 + X1:X2:X3`, etc.\n",
    "\n",
    "`A*B` is a common short-hand for  `A + B + A:B`.\n",
    "\n",
    "How patsy translates a formula can be checked as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44932add-b649-4c6d-a88f-421549bc9481",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from patsy import ModelDesc\n",
    "print(ModelDesc.from_formula('Y ~ 1 + A').describe())\n",
    "print(ModelDesc.from_formula('Y ~ A - 1').describe())\n",
    "print(ModelDesc.from_formula('Y ~ A * B').describe())\n",
    "print(ModelDesc.from_formula('Y ~ A**2').describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb874ed-ea52-44d6-9cec-008be52945a7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Implementation detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fdcfa3-fe7a-48f3-9a2f-b01723a6e440",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "C = 0 # any not-callable value, or worse: an actual function\n",
    "\n",
    "fitted_model = smf.ols('Y ~ C(Group)', data=dataframe).fit()\n",
    "# raises:\n",
    "\n",
    "# >>> PatsyError: Error evaluating factor: TypeError: 'int' object is not callable\n",
    "# >>>    Y ~ C(Group)\n",
    "# >>>        ^^^^^^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a62c11-5794-42a4-9c7e-160babc13731",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "Per default, patsy evaluates the terms in the caller namespace (here the global namespace), so that we can apply local functions to variables right in the formulae.\n",
    "A common usage consists of calling `np.log`.\n",
    "\n",
    "However, this may conflict with previously defined object names, especially in sandbox environment such as a notebook...\n",
    "\n",
    "This default behavior can be disabled with `eval_env=-1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e4e71e-27a4-4c96-ab41-050f4f7597d6",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fitted_model = smf.ols('Y ~ C(Group)', data=dataframe, eval_env=-1).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c895a7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Other issue: if a variable name is a Python reserved keyword (*e.g.* `yield`), the variable must be renamed; there is no other workarounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e28a7a-e821-480b-afd7-8604611f4185",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "## Two-way ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387ae6c6-5458-44aa-8253-5132e59b60a4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let us borrow and adapt the following data example from [statology](https://www.statology.org/two-way-anova-python/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d947388-de44-4f13-b2c0-e8b392856b79",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plant_data = pd.DataFrame({'water': np.repeat(['daily', 'weekly'], 15),\n",
    "                   'sun': np.tile(np.repeat(['low', 'med', 'high'], 5), 2),\n",
    "                   'height': np.array([\n",
    "                       6.3, 6.8, 5.5, 5.1, 6.0, 6.1, 5.0, 6.1, 3.6, 5.4,\n",
    "                       6.4, 5.7, 8.3, 7.7, 7.0, 2.9, 3.2, 2.3, 3.9, 4.1,\n",
    "                       3.5, 5.3, 5.8, 4.6, 3.6, 5.2, 6.2, 5.1, 6.7, 7.0,\n",
    "                   ])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea0c1f5-fb3f-4110-b824-9a7c6018c35e",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "statsmodels_material.illustration_2way_data(plant_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c99a247",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note: we will treat sun exposure as a *cardinal* variable and disregard the natural order of the levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e908f-2c5f-4ec3-9eb4-8cb67f75a2f6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plant_model = smf.ols('height ~ water + sun', data=plant_data).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905ce01a-84fd-49a1-aae8-f1b66a3dd320",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Again, we can use [anova_lm](https://www.statsmodels.org/stable/generated/statsmodels.stats.anova.anova_lm.html) to print a condensed table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc06b3ed-a066-4de3-884c-a7c82729c359",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sm.stats.anova_lm(plant_model, typ=3) # `typ` specifies the type of sum of squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb6951-98ae-4c7b-8919-d9a9475271aa",
   "metadata": {},
   "source": [
    "Here, `anova_lm` prints more useful information than the omnibus statistic given by `summary`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff91aa79-fb03-4b42-9129-8d5abd2add43",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(plant_model.summary().tables[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309fa360-4a14-4f5e-bbe3-eea72e1e8dc4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we look at the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36130c37-c833-4953-9983-7c92bfdbe2e6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plant_model.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fc1dcd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can see the intercept now represents the `water=daily,sun=high` group, and -- for example -- coefficent `water[T.weekly]` encodes the difference between the intercept and the `water=weekly,sun=high` group.\n",
    "\n",
    "Let us reconstruct the group means using the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4785b3-b798-416a-9a53-1d5b873117ce",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ax = sns.swarmplot(data=plant_data, x='sun', y='height', hue='water', dodge=True)\n",
    "\n",
    "x = ax.get_xticks()\n",
    "dx = .2\n",
    "w = plant_model.params\n",
    "\n",
    "y_low_daily = w['Intercept'] + w['sun[T.low]']\n",
    "y_low_weekly = w['Intercept'] + w['sun[T.low]'] + w['water[T.weekly]']\n",
    "ax.plot([x[0]-dx, x[0]+dx], [y_low_daily, y_low_weekly], 'k-d', markerfacecolor='w')\n",
    "\n",
    "y_med_daily = w['Intercept'] + w['sun[T.med]']\n",
    "y_med_weekly = w['Intercept'] + w['sun[T.med]'] + w['water[T.weekly]']\n",
    "ax.plot([x[1]-dx, x[1]+dx], [y_med_daily, y_med_weekly], 'k-d', markerfacecolor='w')\n",
    "\n",
    "y_high_daily = w['Intercept']\n",
    "y_high_weekly = w['Intercept'] + w['water[T.weekly]']\n",
    "ax.plot([x[2]-dx, x[2]+dx], [y_high_daily, y_high_weekly], 'k-d', markerfacecolor='w');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8754fe",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can appreciate the equal daily-weekly differences do not quite match the variability across the levels of the `sun` factor. As a result, the group means are not well represented, including that of the group the intercept is supposed to represent.\n",
    "\n",
    "This inter-factor dependence is called an *interaction*.\n",
    "\n",
    "### Interaction\n",
    "\n",
    "To model this interaction, we need an extra term in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60de13e5-b798-4312-8d06-0ef79f480760",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_with_interaction = smf.ols('height ~ water * sun', data=plant_data).fit()\n",
    "# remember `water * sun` is equivalent to `water + sun + water:sun`\n",
    "print(sm.stats.anova_lm(model_with_interaction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6141c9fa-aab9-45d5-91e3-a41e8fe8587d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model_with_interaction.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de4c0a-ada5-4df1-98bc-43bc7f038cc4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ax = sns.swarmplot(data=plant_data, x='sun', y='height', hue='water', dodge=True)\n",
    "\n",
    "x = ax.get_xticks()\n",
    "dx = .2\n",
    "w = model_with_interaction.params\n",
    "\n",
    "y_low_daily = w['Intercept'] + w['sun[T.low]']\n",
    "y_low_weekly = w['Intercept'] + w['sun[T.low]'] + w['water[T.weekly]'] + w['water[T.weekly]:sun[T.low]']\n",
    "ax.plot([x[0]-dx, x[0]+dx], [y_low_daily, y_low_weekly], 'k-d', markerfacecolor='w')\n",
    "\n",
    "y_med_daily = w['Intercept'] + w['sun[T.med]']\n",
    "y_med_weekly = w['Intercept'] + w['sun[T.med]'] + w['water[T.weekly]'] + w['water[T.weekly]:sun[T.med]']\n",
    "ax.plot([x[1]-dx, x[1]+dx], [y_med_daily, y_med_weekly], 'k-d', markerfacecolor='w')\n",
    "\n",
    "y_high_daily = w['Intercept']\n",
    "y_high_weekly = w['Intercept'] + w['water[T.weekly]']\n",
    "ax.plot([x[2]-dx, x[2]+dx], [y_high_daily, y_high_weekly], 'k-d', markerfacecolor='w');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b2d2f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "statsmodels features an `interaction_plot` helper function, but it does not play nicely with seaborn's `swarmplot` for example and, as a result, we wrap it into another function (see the *statsmodels_material.py* file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811df467-f3ec-4ae4-939c-a8917c2f9d04",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "statsmodels_material.interaction_plot(plant_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846a640e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Treating interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a6451f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As we found significant interaction, we should rerun the ANOVA in the shape of one-way ANOVA, with one factor, for each level of the other factor, and possibly vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe8ac1f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<table><tr><td><img src=\"../images/two-way-anova-interaction-significant-flowchart.png\" /></td></tr>\n",
    "<tr><td><a href=\"https://www.spss-tutorials.com/spss-two-way-anova-interaction-significant/\">SPSS recommendation for two-way ANOVA interaction</a></td></tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c380d42c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "daily_water_model  = smf.ols('height ~ sun',   data=plant_data[plant_data['water']=='daily']).fit()\n",
    "weekly_water_model = smf.ols('height ~ sun',   data=plant_data[plant_data['water']=='weekly']).fit()\n",
    "low_sun_model      = smf.ols('height ~ water', data=plant_data[plant_data['sun']=='low']).fit()\n",
    "med_sun_model      = smf.ols('height ~ water', data=plant_data[plant_data['sun']=='med']).fit()\n",
    "high_sun_model     = smf.ols('height ~ water', data=plant_data[plant_data['sun']=='high']).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39e6bd5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If main effects are found to be significant, we can proceed to performing post-hoc tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ccdd87",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "daily_water_model.f_pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1392464",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "daily_water_posthoc = daily_water_model.t_test_pairwise('sun')\n",
    "daily_water_posthoc.result_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8724bf04",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weekly_water_model.f_pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc3a0ad",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weekly_water_posthoc = weekly_water_model.t_test_pairwise('sun').result_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98126f2f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Problem: although `t_test_pairwise` includes a correction for multiple comparisons, this correction does not account for the multiple calls to `t_test_pairwise` we perform on the same data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48953eb2-9d8b-42e4-b086-144f07a54359",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "## Post-hoc tests and multiple comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d4382a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### The multiple comparisons problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f10e1-d677-4ef6-b53c-7c80f8cf64ea",
   "metadata": {},
   "source": [
    "Let us perform 1,200 tests whom 100 should lead to a significant different. The test used features the following properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f395588",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "power = 0.8\n",
    "type1_error_rate = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167951da",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Red pixels represent the tests (comparisons) for which $H_0$ is false (right figure) or rejected (left figure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8642f0e6",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "statsmodels_material.illustration_multiple_comparisons(power, type1_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdba34b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The top right group of tests with false $H_0$ is affected by type-2 errors, or equivalently by the power of the test (`power = 1 - type2_error_rate`).\n",
    "\n",
    "The original $5%$ significance level of the test translates into $5%$ type-1 errors that become visible when the test is applied many times as we did above.\n",
    "\n",
    "We want the significance level to apply to the \"whole picture\", and to control the *family-wise error rate*. Basically, we want our $5%$ level to upper-bound the risk of erroneously rejecting any single $H_0$ (or more).\n",
    "\n",
    "This can be done with a procedure called *correction for multiple comparisons*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d0f081",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### multipletests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ea24c3",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we consider all 5 factored models, we may proceed to performing up to 9 comparisons, but again `t_test_pairwise` would not properly take this into account.\n",
    "\n",
    "Note that you do not need to perform all possible comparisons. Choose what comparisons you are interested in, but do so prior to performing them.\n",
    "\n",
    "We should use [multipletests](https://www.statsmodels.org/stable/generated/statsmodels.stats.multitest.multipletests.html) instead, for the purpose of correcting the $p$-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9e153d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "significance_level = 0.05\n",
    "\n",
    "all_comparisons = []\n",
    "for factor1, factor2 in (('sun', 'water'), ('water', 'sun')):\n",
    "    for f2_level in np.unique(plant_data[factor2]):\n",
    "        model = smf.ols(f'height ~ {factor1}', data=plant_data[plant_data[factor2]==f2_level]).fit()\n",
    "        if model.f_pvalue <= significance_level:\n",
    "            model_name = f'{factor2}={f2_level}'\n",
    "            pairwise_tests = model.t_test_pairwise(factor1).result_frame\n",
    "            pairwise_tests.index = [ f'{comparison}[{model_name}]' for comparison in pairwise_tests.index ]\n",
    "            all_comparisons.append(pairwise_tests)\n",
    "all_comparisons = pd.concat(all_comparisons)\n",
    "\n",
    "len(all_comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe99f26",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "all_comparisons['reject-hs'], all_comparisons['pvalue-hs'], _, _ = multipletests(all_comparisons['P>|t|'], alpha=significance_level)\n",
    "all_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b21f28-f70d-466e-bbb6-8a6fc86b3696",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "statsmodels_material.confidence_intervals(all_comparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748acdc4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Beware: the confidence intervals from [t_test_pairwise](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLSResults.t_test_pairwise.html)'s table are not corrected for multiple comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f436837b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Bonferroni, Šidák and Holm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecb5d85-20d7-4261-a572-b6c891cb8aff",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[`multipletests`](https://www.statsmodels.org/stable/generated/statsmodels.stats.multitest.multipletests.html) implements several correction procedures. The Holm correction with Šidák adjustments is the default method (`holm-sidak`).\n",
    "\n",
    "If we perform $n$ tests, the $p$-value for each test can be adjusted as follows:\n",
    "\n",
    "* Bonferroni adjustement : $p_{corrected} = np$\n",
    "* Šidák adjustment : $p_{corrected} = 1 - ( 1 - p )^n$\n",
    "\n",
    "In the Holm's procedure, we sequentially consider each $p$-value, starting from the smallest one, and adjust them on basis of the number of remaining $p$-values to adjust.\n",
    "Basically:\n",
    "\n",
    "1. the smallest $p$-value is adjusted considering $n$ multiple comparisons (because we have not adjusted any $p$-value yet),\n",
    "2. the second smallest $p$-value is adjusted considering $n-1$ multiple comparisons (because we have already adjusted one $p$-value),\n",
    "3. and so on.\n",
    "\n",
    "Compared with pingouin's [`multicomp`](https://pingouin-stats.org/build/html/generated/pingouin.multicomp.html#pingouin.multicomp), statsmodels' `multipletests` includes more False Discovery Rate (FDR)-based correction methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1da0e-1bd1-42af-8a6e-c4effe40cef0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Types of sums of squares\n",
    "\n",
    "`anova_lm` takes an argument `typ` that can be any of `1`, `2` and `3`.\n",
    "\n",
    "Indeed, to quantify the contribution of each term to the model, we can choose between three ways of decomposing the total variance or sum of squares.\n",
    "\n",
    "Let us consider the following model: `Y ~ A + B + A:B`\n",
    "\n",
    "#### Type-1\n",
    "\n",
    "* A's contribution will evaluated comparing `Y ~ A` vs `Y ~ 1`\n",
    "* B: `Y ~ A + B` vs `Y ~ A`\n",
    "* A:B (interaction term): `Y ~ A + B + A:B` vs `Y ~ A + B`\n",
    "\n",
    "#### Type-2\n",
    "\n",
    "* A: `Y ~ A + B` vs `Y ~ B`\n",
    "* B: `Y ~ A + B` vs `Y ~ A`\n",
    "* A:B: `Y ~ A + B + A:B` vs `Y ~ A + B`\n",
    "\n",
    "Type-2 is often chosen for regression problems (with continuous predictors).\n",
    "\n",
    "#### Type-3\n",
    "\n",
    "* A: `Y ~ A + B + A:B` vs `Y ~ B + A:B`\n",
    "* B: `Y ~ A + B + A:B` vs `Y ~ A + A:B`\n",
    "* A:B: `Y ~ A + B + A:B` vs `Y ~ A + B`\n",
    "\n",
    "Type-3 is suitable for multi-factorial designs (with several categorical factors) and unbalanced groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc143f7-d3dd-4b19-ae05-9b639b1baf7c",
   "metadata": {},
   "source": [
    "## Mixed-effects models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2911be-e687-4170-8422-75a195b1cf84",
   "metadata": {},
   "source": [
    "Let us consider the reaction-time dataset available at [osf.io/asq8n](https://osf.io/asq8n):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e203a8d-a9d5-43e0-a437-918d7e036aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_data = pd.read_csv('https://osf.io/download/asq8n/')\n",
    "rt_data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399ab879-04f4-440d-85c3-c3e6d773b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.mixedlm('utterancelength ~ place * gender', rt_data, groups='subject').fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbc0d33-5d7f-4942-817f-47faf5c5ca50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.mixedlm('utterancelength ~ place * gender', rt_data, groups='subject', vc_formula={'item': '0 + item'}, re_formula='1').fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ea99ae-96a7-45cd-98fe-1ad67b131978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sm.stats.anova_lm(model) # does not work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87ffec5-0127-45f2-8d73-75819c1cd3ff",
   "metadata": {},
   "source": [
    "Instead of the traditional sums-of-squares and ANOVA, we test for main effects by comparing models using the Wald test.\n",
    "\n",
    "We compare the full model with a model without the effect of interest. This second model is specified as a constraint on one or more model coefficients. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac0c5c-93d6-4221-9be4-c601583ba295",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wald_test('gender[T.male] = 0', scalar=True, use_f=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81507156-c9c7-4d20-b48d-5f1669ec21b4",
   "metadata": {},
   "source": [
    "In the above example, we have established that the `gender` factor has no effects (`p-value>0.05`).\n",
    "\n",
    "The $p$-values reported by `summary` on coefficients associated with binary factors readily inform about the factor's effect.\n",
    "\n",
    "Multi-level main effects can be tested with null hypothesis: *all related coefficients equal to $0$*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b122d238-a7a3-4203-86c5-db96eb229fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wald_test('place[T.velar] = place[T.labial] = 0', scalar=True, use_f=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258bc481-0296-4493-8e6e-ebd17c8886e7",
   "metadata": {},
   "source": [
    "If a factor or interaction term does not exhibit an effect, it can be removed from the model. This is often done for interaction terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afad7ba-e03b-47cf-8d60-4f2cd9dacfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wald_test('place[T.labial]:gender[T.male] = place[T.velar]:gender[T.male] = 0', scalar=True, use_f=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b6b036-9062-4b27-8737-0f7e3d86ad9d",
   "metadata": {},
   "source": [
    "Instead of posthoc tests, one can test individual pairwise differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9058d6-6110-49e3-a3af-8af9d4c7cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wald_test('place[T.velar] = place[T.labial]', scalar=True, use_f=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd7dd0-9c93-427f-b254-8098bd95e3c3",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929d319e",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "What if -- instead of factors -- our independent variables are continuous variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bf953c",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b350dec1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "patients = pd.read_csv('../data/patients.csv')\n",
    "patients.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e86aec",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=patients, x='CHUK', y='Response', label='Patient');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd68586",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = smf.ols('Response ~ CHUK', patients).fit()\n",
    "#print(model.summary().tables[0])\n",
    "print(model.summary().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634c5b3f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<p style=\"font-size: x-small;\">Data set and choice of an explanatory variable inspired by the RS3 session about linear models on <a href=\"https://moodle01.hosting.pasteur.fr\">Institut Pasteur's Moodle</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50155198",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "statsmodels_material.illustration_regression(patients, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215fb86c-3a14-45b7-ad65-9cfeafc410d4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65111a8d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Similarly to previous OLS applications, the coefficients of the model can be found in the `coef` column.\n",
    "\n",
    "$\n",
    "\\texttt{Response} = a + b\\mbox{ }\\texttt{CHUK} + \\epsilon\n",
    "$\n",
    "\n",
    "with intercept $a = -11.2792$ and slope $b = 0.9727$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a88dd",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Residual plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bbd0e7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To assess the adequacy of the model, we inspect the residuals $\\epsilon_i$ in various.\n",
    "First, we plot the residuals *vs* the explanatory variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95319ec8-320b-4fbb-a62e-a3665b40b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc471543",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "statsmodels_material.illustration_regression_residuals(patients, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15946b2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The most distant points may be outliers.\n",
    "\n",
    "We expect the residuals not to exhibit any structure:\n",
    "\n",
    "* systematic (positive-only or negative-only) errors on subdomains of the explanatory variable are indicative of the model not being flexible enough,\n",
    "* the dispersion of the residuals should not vary as a function of the explanatory variable (homoscedasticity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce469df",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<table width=60%><tr><td><img src=\"../images/heteroskedasticity.png\" /></td></tr>\n",
    "<tr><td><a href=\"https://towardsdatascience.com/heteroscedasticity-is-nothing-to-be-afraid-of-730dd3f7ca1f\">\"Heteroscedasticity is nothing to be afraid of\" - Sachin Date</a></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777daba8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Criterion: the residuals should be normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7780a9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sm.graphics.qqplot(model.resid, fit=True, line='45', fmt='b', marker='+');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59367b93",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note: statsmodels's `qqplot` compares with R's qqplot as long as `fit=True` to standardize the residuals.\n",
    "`fit=True` makes `qqplot` differs from scipy's `probplot`.\n",
    "\n",
    "In what refers to normality of the residuals, the last summary table is also informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5492f4e8-2ac8-4ba7-9a6c-241f33994998",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(model.summary().tables[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f809c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The hypothesis of normality of the residuals is rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8cf02d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Influence plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345ad8a8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To identify outliers and influential points, statsmodels features more [diagnostic measures and plots](https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.OLSInfluence.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79b930e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "diagnostics = OLSInfluence(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab691b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "diagnostics.summary_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f143230",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1, 2, figsize=(13.3,4.1))\n",
    "\n",
    "diagnostics.plot_influence(ax=axes[0])\n",
    "axes[0].axhline(0, linestyle=':', linewidth=1)\n",
    "\n",
    "diagnostics.plot_index(threshold=0.02, ax=axes[1])\n",
    "axes[1].axhline(0, linestyle=':', linewidth=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d44448",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Leverage and Cook's distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a8f174",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let us consider a smaller data sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc655fc0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(237598)\n",
    "x = stats.lognorm.rvs(1, size=30)\n",
    "y = np.log(4 + x + stats.norm.rvs(size=x.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b46df6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=x, y=y)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b5ffc9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.regplot(x=x, y=y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb8d264",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = np.stack((np.ones_like(x), x), axis=1)\n",
    "model = sm.OLS(y, X).fit()\n",
    "diagnostics = OLSInfluence(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddf4e63",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1, 2, figsize=(13.3,4.1))\n",
    "\n",
    "diagnostics.plot_influence(ax=axes[0])\n",
    "axes[0].axhline(0, linestyle=':', linewidth=1)\n",
    "diagnostics.plot_index(threshold=0.02, ax=axes[1])\n",
    "axes[1].axhline(0, linestyle=':', linewidth=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a441682",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "high_leverage_point = np.argmax(diagnostics.hat_matrix_diag) # 20\n",
    "cooks_distant_point = np.argmax(diagnostics.cooks_distance[0]) # 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8019bc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "statsmodels_material.illustration_outlier(x, y, high_leverage_point, cooks_distant_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b9eb6d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The leverage for a point tells how much the model would change if we move the response value of that point, while Cook's distance reflects how much the model changes if we omit the point.\n",
    "\n",
    "Therefore, Cook's distance is an “effect size” for outliers. Influential points that fall above $1$ are undesirable and should preferably be removed or trimmed (see also [robust linear models](https://www.statsmodels.org/stable/generated/statsmodels.robust.robust_linear_model.RLM.html)). A Cook's distance between $0.5$ and $1$ signals a point (=an observation) to be examined.\n",
    "\n",
    "Note: compared to other implementations of influence plots, statsmodels' influence plot lacks the Cook's distance isocurves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e0be6f",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "## Non-linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73598e37",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b229d5e0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the previous simulated data example, the relationship between the explanatory and response variables is actually not linear. The true model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b96410",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sns.regplot(x=np.log(x), y=y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433fbffe",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Such simple non-linear relationships are common and a useful trick consists in transforming the explanatory variables using monotonous functions.\n",
    "\n",
    "Examples (heavily inspired by the RS3 session about linear models on [Institut Pasteur's Moodle](https://moodle01.hosting.pasteur.fr)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709f2da0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "statsmodels_material.illustration_monotonous_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c3ba5c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Generic standardization functions exist, such as the [Box-Cox transform](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html) in scipy, but they often require the explanatory variable to take positive values and the interpretation of the relationship becomes less straight-forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5150655f",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd56cd7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let us consider some almost-linearly related data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02debff3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_sample(n=100):\n",
    "    # we will use this code again later in the notebook\n",
    "    x = np.sort(stats.uniform.rvs(0, 2, size=n))\n",
    "    y_th = 1 + 0.5 * x + 1.5 * x**2 + 0.3 * x**3\n",
    "    y = y_th + 2 * stats.norm().rvs(size=x.size)\n",
    "    return x, y, y_th\n",
    "\n",
    "np.random.seed(237598)\n",
    "x, y, y_th = get_sample()\n",
    "\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "\n",
    "ax = sns.scatterplot(x='x', y='y', data=df, label='observations')\n",
    "\n",
    "#ax.plot(x, y_th, 'r-', label='true relationship') # uncomment\n",
    "\n",
    "ax.set_xlabel('$x$ (explanatory)')\n",
    "ax.set_ylabel('$y$ (response)')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cc46cf",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A linear model performs very well, but the structured errors leave no doubt the model is not flexible enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32746edb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "linear_model = sm.OLS.from_formula('y ~ x', df).fit()\n",
    "\n",
    "_, axes = plt.subplots(1, 2, figsize=(13.3,4.1))\n",
    "sns.regplot(x='x', y='y', data=df, ax=axes[0], line_kws=dict(color='r', linewidth=1.5))\n",
    "sns.scatterplot(x='x', y='residuals', data=df.assign(residuals=linear_model.resid), ax=axes[1])\n",
    "axes[1].axhline(0, linestyle=':', linewidth=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a8c0e8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A flexible approach consists of introducing powers of the explanatory variable, in the shape of multiple data columns.\n",
    "\n",
    "$$\n",
    "Y = \\left[ X^0, X^1, X^2, ... \\right]\\beta + \\epsilon\n",
    "$$\n",
    "or similarly $y_i = \\beta_0 + \\beta_1 x_i +\\beta_2 x_i^2 +... +\\epsilon_i$ for all observation $i$.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3edfab",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "augmented_df = df.assign(x2 = x**2)[['y', 'x', 'x2']]\n",
    "augmented_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c100a8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "poly2_model = smf.ols('y ~ 1 + x + x2', augmented_df).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761ebbca",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As the predictors we plug into the model are synthetic, we do not need to model any interaction between them.\n",
    "\n",
    "Similarly, we can manually define the `exog` matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac4ac8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_poly2 = np.stack((np.ones_like(x), x, x*x), axis=1)\n",
    "poly2_model_bis = sm.OLS(y, X_poly2).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078047e2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "statsmodels_material.illustration_nonlinear_regression(df, y_th, poly2_model, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97671b61",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Polynomial models are flexible enough to closely approximate any function in the neighborhood of a point (think of Taylor series expansions) but, of course, may not be adequate enough as we are modelling a function across an entire domain.\n",
    "\n",
    "It is also possible to introduce any other non-linear transformation of the explanatory variable as additional terms in the modelling equation or columns in the design matrix. See the following examples in statsmodels documentation: [1](https://www.statsmodels.org/stable/examples/notebooks/generated/predict.html) [2](https://www.statsmodels.org/stable/examples/notebooks/generated/ols.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e27d4",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ccd181",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let us introduce higher order terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba76906",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "augmented_again_df = augmented_df.assign(x3=x**3, x4=x**4, x5=x**5, x6=x**6) # yippee!\n",
    "poly6_model = smf.ols('y ~ 1 + x + x2 + x3 + x4 + x5 + x6', augmented_again_df).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424e6080",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "statsmodels_material.illustration_nonlinear_regression(df, y_th, poly6_model, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c07909",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we compare the various models, we can observe that more complex models tend to perform better on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c19b10",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(np.array(\n",
    "    [[model.rsquared, model.rsquared_adj, model.llf, model.aic, model.bic] \\\n",
    "        for model in (linear_model, poly2_model, poly6_model)]),\n",
    "    index=['1', '2', '6'],\n",
    "    columns=['R2', 'R2_adjusted', 'log-likelihood', 'AIC', 'BIC'])\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648b703e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now if we get a new sample from the same population, and compute the coefficient of determination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c8cdf3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_test, y_test, _ = get_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3131de8c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "statsmodels_material.illustration_R2_poly(x, y, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a2fd9f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "...the over-complex models perform poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aafcbf",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e112ca",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Choosing among models should not rely on data fitness only, especially if we only consider the data used to fit the model.\n",
    "\n",
    "To choose between models, two strategies:\n",
    "\n",
    "* model evaluation on test data, *i.e.* a second (sub-)sample drawn from the same population as the data used to fit the model,\n",
    "    * => `scikit-learn`\n",
    "* heuristics; for example, model complexity is to be controlled, so that simpler models are favored over complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f85206",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Information criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31a31c9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Akaike (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion) and [Bayesian (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion) information criteria combine model fitness with the notion of model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ac886e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b40681",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$\n",
    "AIC = 2k - 2\\log{L}\n",
    "$$\n",
    "$$\n",
    "BIC = k\\log{n} - 2\\log{L}\n",
    "$$\n",
    "\n",
    "with $\\log{L}$ the maximum log-likelihood we also met in the OLS summary, and quantifies the goodness-of-fitness.\n",
    "\n",
    "$k$ is the number of estimated parameters in the model, and $n$ the number of observations.\n",
    "For $p$ predictors (explaining variables), a linear model's $k=p+2$ because we also estimate an intercept and the error variance.\n",
    "\n",
    "The likelihood is the probability that the data are generated by the model: $L=P\\left(X,y|\\mathcal{M}(\\theta)\\right)$, denoting $\\mathcal{M}(\\theta)$ the model with parameters $\\theta$ (estimated coefficients).\n",
    "\n",
    "In the case of a linear regression with normally-distributed residuals: $\\log{L}\\propto -\\frac{\\sum_i(y_i - \\textbf{x}_i^\\top\\beta)^2}{2\\sigma^2}$ with $\\beta$ are the regression coefficients.\n",
    "\n",
    "`OLS` models rely on such a form for the likelihood and that is the reason why we must ensure the residuals are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60322cd4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "statsmodels_material.illustration_AIC_BIC_poly(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74521465",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In a linear model of several factors and many potential interactions terms, the AIC is often used to compare between all the possible models and select a model that exhibits a good trade-off between the likelihood (higher is better) and the number of terms (fewer is better)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc7280e-2dd2-49bd-af85-de6538245cc0",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "## Generalized linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d397a38",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What if -- instead of a continuous variable -- our response variable is a binary (or categorical) variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f7ce4-9acf-461a-a9a3-2a78b2af2e01",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "### Link functions and families"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982d6488",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We need two ingredients.\n",
    "\n",
    "First, we apply a transformation to the response variable to project its values onto $\\mathbb{R}$:\n",
    "\n",
    "$$\n",
    "g(y) = \\textbf{X}\\beta + \\epsilon\n",
    "$$\n",
    "\n",
    "$g$ is called a *link function*. \n",
    "\n",
    "At the same time, a *family* of distribution functions is chosen for $y|\\textbf{X}$, to express $\\mathbb{E}[y|\\textbf{X}] = g^{-1}(\\textbf{X}\\beta)$. Importantly, the chosen distribution determines the relationship between the mean and variance of $y|\\textbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c374bbc5",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "### Example problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c0391c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Count outcome\n",
    "\n",
    "\\[copying [Wikipedia](https://en.wikipedia.org/wiki/Generalized_linear_model)\\]\n",
    "Suppose we want to predict how many people will come to some type of outdoor places (*e.g.* beaches) as a function of temperature.\n",
    "\n",
    "If we observed the attendance in multiple occasions, mostly at temperatures in the 15-35°C range, a fitted linear model could predict impossible values, namely negative attendance, at low temperatures, say 5°C.\n",
    "\n",
    "A link function could be used to turn the attendance into a variation rate of attendance, so that the attendance can be multiplied or divided as a function of temperature increase/decrease, and never subtracted.\n",
    "\n",
    "To do so, we need an *exponential response* model with $g=\\log$. If $g(y)$ varies linearly with temperature, $y$ will be Poisson distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c906df73",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "family = sm.families.Poisson()\n",
    "# an optional first argument specifies the link function; default is the log link function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fe5847",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Odds ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2523f9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose now we want to model the probability of a two-option process, *e.g.* «survives *vs* dies».\n",
    "\n",
    "A response variable is always quantitative. If we choose it to be the raw probability, we may find situations such that the predicted probability takes a negative value, or falls above $1$.\n",
    "\n",
    "If we think of the effect of a change in the value of an explanatory variable, this effect better applies to the *odds* of survival, *i.e.* the ratio of survival over death, and again we want the model to make this value vary in a multiplicative way (no subtraction, no addition).\n",
    "\n",
    "This is a typical application of a *logit* link function, which relies on $y$ following a binomial distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42165fc7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "family = sm.families.Binomial()\n",
    "# the default link function is logit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c272448",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "### Choosing families and link functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791c1c48",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Common choices are:\n",
    "* $y$ is continuous in $\\mathbb{R}$: Gaussian distribution with identity link function (standard linear model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf2d971",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "family = sm.families.Gaussian()\n",
    "# the default link function is the identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70d80de",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* $y$ is continuous in $\\mathbb{R}^+$: gamma or inverse Gaussian distribution with log link function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c495f24",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "family = sm.families.Gamma(sm.families.links.Log())\n",
    "family = sm.families.InverseGaussian(sm.families.links.Log())\n",
    "# the log link function must be specified, as the log link function is not default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841be7c7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* $y$ is count data ($y \\in \\mathbb{N}^+$): Poisson distribution (see above)\n",
    "* $y$ is $0$ or $1$: binomial distribution with *logit* link function (see above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b867c0f0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e6b10d-fc3a-4fe8-9f94-b3ff3a1278ea",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/titanic.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959c8a9",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = smf.glm('Survived ~ Age + C(Pclass) + C(Sex)', df, family=sm.families.Binomial())\n",
    "model = model.fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3b079e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Some general statistics are not shown in the tables above, but still avaible in the *Generalized Linear Model Regression Results*  object, such as the AIC or BIC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797dabea",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.aic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5fbbc1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Main effects and pairwise differences can be tested using Wald test as already shown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec030ec2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Repeated-measures ANOVA and sphericity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f42e4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Example (one-way): each animal observed multiple times, *e.g.* at different ages; and we are not interested in the putative differences between animals.\n",
    "\n",
    "$$\n",
    "SS_{\\textrm{total}} = SS_{\\textrm{treatment}} + (SS_{\\textrm{subject}} + SS_{\\textrm{error}})\n",
    "$$\n",
    "\n",
    "$$\n",
    "F^* = \\frac{\\frac{SS_{\\textrm{treatment}}}{k - 1}}{\\frac{SS_{\\textrm{error}}}{(k-1)(n-1)}}\n",
    "$$\n",
    "\n",
    "Designs are balanced.\n",
    "\n",
    "Let us borrow an example from `pingouin` documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea2319c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "\n",
    "data = pg.read_dataset('rm_anova2')\n",
    "data.loc[[0,1,10,11,20,21,30,31]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdbf3b2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this example, each subject (`Subject`) has undergone all possible measurements, for all levels of the `Time` and `Metric` factors.\n",
    "As a consequence, the observations for each subject are not independent, and this must be accounted for by the model.\n",
    "\n",
    "In a standard repeated measures ANOVA, the covariance structure is just assumed to exhibit a property called sphericity.\n",
    "\n",
    "`Time` and `Metric` are called *within-subject* factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831d8c07",
   "metadata": {
    "hidden": true
   },
   "source": [
    "statsmodels features [AnovaRM](https://www.statsmodels.org/stable/generated/statsmodels.stats.anova.AnovaRM.html) but corrections for departure from sphericity are not implemented and we should first perform a Mauchly's test for sphericity, for example with [pingouin.sphericity](https://pingouin-stats.org/generated/pingouin.sphericity.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df0d24",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pg.sphericity(data, dv='Performance', subject='Subject', within=['Time', 'Metric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8362eb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats import anova\n",
    "result = anova.AnovaRM(data, depvar='Performance', subject='Subject', within=['Time', 'Metric']).fit()\n",
    "result.anova_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb44cf31",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In contrast, [rm_anova](https://pingouin-stats.org/generated/pingouin.rm_anova.html) from pingouin does implement Greenhouse-Geiser correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66cb913",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pg.rm_anova(data, dv='Performance', subject='Subject', within=['Time', 'Metric'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd20d1c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Mixed effects models are increasingly popular and preferred over the standard repeated measures ANOVA, especially because sphericity simply cannot be expected from the data in most cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scientific_python",
   "language": "python",
   "name": "scientific_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
